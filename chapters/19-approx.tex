%&../settings/preamble.main

% NOTE per controllare dove sfori la pagina
% \setlength\overfullrule{5pt}

% NOTE stampa della sottosottosezione
% \setcounter{tocdepth}{3}
% \tableofcontents

\ifloadsubpreamble
\pagestyle{plain}
\setcounter{chapter}{18}
\fi

% arara: pdflatex: { draft: yes, synctex: no }
% arara: pdflatex: { synctex: no }
% arara: latexmk:  { clean: partial }
\begin{document}

\chapter{Soluzioni per problemi intrattabili}

\section{Algoritmi pseudo-polinomiali}

Non si può avere tutto dalla vita; bisogna rinunciare a qualcosa:
\begin{itemize}
	\item \textbf{generalità}: algoritmi pseudo-polinomiali che funzionano per solo alcuni casi particolari dell'input;
	\item \textbf{ottimalità}: algoritmi di approssimazione, che garantiscono di ottenere soluzioni \enquote{vicine} alla soluzione ottimale;
	\item \textbf{formalità}: algoritmi euristici, di solito basati su tecniche \foreign{greedy} o di ricerca locale, che forniscano sperimentalmente risultati buoni;
	\item \textbf{efficienza}: algoritmi esponenziali branch-\&-bound, che limitano lo spazio di ricerca con un'accurata potatura.
\end{itemize}

\subsection{Somma di sottoinsiemi {\subSetSumProblem}}

\paragraph{Definizione del problema}
Dati un insieme \(A = \{a_1, a_2, \dots, a_n\}\) di interi positivi e un intero positivo \(k\), esiste un sottoinsieme \(S\) di indici in \(\{1, \dots, n\}\) tale che \(\sum_{i \in S}^{a_i} = k\) ?

Utilizzando \foreign{backtracking}, abbiamo risolto la versione di ricerca di questo problema.
Quella appena enunciata è la versione decisionale.
Per semplificare il confronto, ci concentriamo sulla seconda.

\subsubsection{Somma di sottoinsiemi risolto tramite programmazione dinamica}

Definiamo una tabella booleana \(DP[0 \dots n][0 \dots k]\).
\(DP[i][r]\) è uguale a \True se e solo se è possibile ottenere \(r\) dai primi \(i\) valori memorizzati nel vettore di input.
\[
	DP[i][r] =
	\begin{dcases}
		\True                                         & r = 0 \\
		\False                                        & r > 0 \land i = 0\\
		DP[i-1][r]                                    & r > 0 \land i > 0 \land A[i] > r\\
		DP[i - 1][r] \text{ \Or } DP[i - 1][r - A[i]] & r > 0 \land i > 0 \land A[i] \leqslant r\\
	\end{dcases}
\]

Essendo un problema decisionale, è possibile semplificare e utilizzare spazio \(\Theta(k)\), invece che \(\Theta(nk)\), in quanto avrò \(n\) righe e \(k\) colonne.
L'algoritmo sfrutta l'equazione di ricorrenza.

\begin{algorithm}[H]
	\caption{Somma di sottoinsiemi risolto tramite programmazione dinamica}
	\input{subSetSum}
\end{algorithm}

Ad esempio prendendo l'insieme \(A = [5,9,10]\) di interi positivi e l'intero positivo \(k = 24\) proviamo a riempire la tabella di programmazione dinamica.
Dove le righe rappresentano gli indici presi e le colonne il valore di \(k\) desiderato.

\begin{table}[H]\centering
	\begin{tabular}{@{} c *{25}{@{\hskip 8pt}c} @{}}
		\toprule
			& \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} & \textbf{12} & \textbf{13} & \textbf{14} & \textbf{15} & \textbf{16} & \textbf{17} & \textbf{18} & \textbf{19} & \textbf{20} & \textbf{21} & \textbf{22} & \textbf{23} & \textbf{24}\\
		\cmidrule{2-26}
			\textbf{0} & 1 &&&&&&&&&&&&&&&&&&&&&&&& \\
		\lightrule
			\textbf{1} & 1 &&&&& 1 &&&&&&&&&&&&&&&&&&& \\
		\lightrule
			\textbf{2} & 1 &&&&& 1 &&&& 1 &&&&& 1 &&&&&&&&&& \\
		\lightrule
			\textbf{3} & 1 &&&&& 1 &&&& 1 & 1 &&&& 1 & 1 &&&& 1 &&&&& 1 \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Analisi della complessità}
La complessità dell'algoritmo è \(\Theta(nk)\), ma la complessità dei dati in ingresso è \(\Omicron(n \log k)\), in quanto i valori più grandi del nostro obiettivo possono essere esclusi.
Se \(k\) è \(\Omicron(n^c)\) con \(c\) costante, allora \subSetSum ha complessità polinomiale \(\Omicron(n^{c+1})\).
Ma se \(k\) è  \(\Omicron(2^n)\), allora \subSetSum ha complessità superpolinomiale \(\Omicron(n \cdot 2^{n})\).

\begin{observation}
La complessità di \subSetSum dipende quindi dai valori contenuti nell'insieme e non soltanto dalla cardinalità dei dati in ingresso (\(n\)).
\end{observation}

\subsubsection{Somma di sottoinsiemi risolto tramite backtracking}

\begin{algorithm}[H]
	\caption{Somma di sottoinsiemi risolto tramite backtracking}
	\input{subSetSumRec}
\end{algorithm}

\begin{table}[H]\centering
	\begin{tabular}{@{} c *{25}{@{\hskip 8pt}c} @{}}
		\toprule
			& \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} & \textbf{12} & \textbf{13} & \textbf{14} & \textbf{15} & \textbf{16} & \textbf{17} & \textbf{18} & \textbf{19} & \textbf{20} & \textbf{21} & \textbf{22} & \textbf{23} & \textbf{24}\\
		\cmidrule{2-26}
			\textbf{0} & 1 &&&&& 0 &&&& 0 & 0 &&&& 0 & 0 &&&&&&&&& 0 \\
		\lightrule
			\textbf{1} &&&&&& 1 &&&&&&&&& 0 & 0 &&&&&&&&& 0 \\
		\lightrule
			\textbf{2} &&&&&&&&&&&&&&& 1 &&&&&&&&&& 0 \\
		\lightrule
			\textbf{3} &&&&&&&&&&&&&&&&&&&&&&&&& 1 \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Complessità}
\(\Omicron(2^n)\).

\subsubsection{Somma di sottoinsiemi risolto tramite memoization}

\begin{algorithm}[H]
	\caption{Somma di sottoinsiemi risolto tramite memoization}
	\input{subSetSumMemo}
\end{algorithm}

\begin{table}[H]\centering
	\begin{tabular}{@{} c *{25}{@{\hskip 8pt}c} @{}}
		\toprule
			& \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} & \textbf{12} & \textbf{13} & \textbf{14} & \textbf{15} & \textbf{16} & \textbf{17} & \textbf{18} & \textbf{19} & \textbf{20} & \textbf{21} & \textbf{22} & \textbf{23} & \textbf{24}\\
		\cmidrule{2-26}
			\textbf{0} & 1 &&&&& 0 &&&& 0 & 0 &&&& 0 & 0 &&&&&&&&& 0 \\
		\lightrule
			\textbf{1} &&&&&& 1 &&&&&&&&& 0 & 0 &&&&&&&&& 0 \\
		\lightrule
			\textbf{2} &&&&&&&&&&&&&&& 1 &&&&&&&&&& 0 \\
		\lightrule
			\textbf{3} &&&&&&&&&&&&&&&&&&&&&&&&& 1 \\
		\bottomrule
	\end{tabular}
\end{table}

Se prendiamo in considerazione un altro esempio con l'insieme \(A = [1,1,1,1,1]\) di interi positivi e l'intero positivo \(k = 5\) proviamo a riempire la tabella di programmazione dinamica

\begin{table}[H]\centering
	\begin{tabular}{@{} c *{6}{@{\hskip 16pt}c} @{}}
		\toprule
			& \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
		\cmidrule{2-7}
			\textbf{0} & 1 & 0 & 0 & 0 & 0 & 0 \\
		\lightrule
			\textbf{1} &   & 1 & 0 & 0 & 0 & 0 \\
		\lightrule
			\textbf{2} &   &   & 1 & 0 & 0 & 0 \\
		\lightrule
			\textbf{3} &   &   &   & 1 & 0 & 0 \\
		\lightrule
			\textbf{4} &   &   &   &   & 1 & 0 \\
		\lightrule
			\textbf{5} &   &   &   &   &   & 1 \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Complessità}
\(\Omicron(nk)\)

\subsubsection*{Discussione sulla complessità di somma di sottoinsiemi}

Riassumendo abbiamo risolto il problema tramite la programmazione dinamica ottenendo una complessità di \(\Theta(nk)\), tramite backtracking ottenendo una complessità di \(\Omicron(2^n)\) ed infine tramite memoization ottenendo una complessità di \(\Omicron(nk)\).

Ma \(\Omicron(nk)\) è una complessità superpolinomiale?
No, non lo è, infatti \(k\) è parte dell'input, non una dimensione dell'input.
\(k\) viene rappresentato da \(t = \ceil{\log k}\) cifre binarie.
Quindi la complessità è \(\Omicron(nk) = \Omicron(n \dots 2^t)\), esponenziale.

\begin{table}[htbp]\centering
	\caption[Complessità di {\subSetSumProblem}]{Di seguito sono riportate le complessità dei vari algoritmi per la risoluzione del problema di somma di sottoinsiemi. Notiamo che nell'applicazione dell'algoritmo di programmazione dinamica, i valori \(k\) contenuti nell'insieme di input influenzano la complessità dell'algoritmo; mentre nell'applicazione dell'algoritmo di memoization i valori fanno parte dell'input, quindi ciò che influenza la complessità è la dimensione della rappresentazione dei dati dell'insieme, non i valori contenuti in esso. Infine nell'applicazione della tecnica di backtracking la complessità non è influenzata dai dati in ingresso.}
	\label{tab:subsetsum-complexity}
	\begin{tabular}{@{} *{5}{l} @{}}
		\toprule
			\multicolumn{5}{c}{{\subSetSumProblem}} \\
		\midrule
			\emph{Tecnica} & \emph{Algoritmo} & \emph{Input} & \emph{Complessità} & \\
		\midrule
			\multirow{2}{*}{Programmazione Dinamica} & \multirow{2}{*}{\(\Theta(nk)\)} & \(k = \Omicron(n^c)\) & \(\Omicron(n^{c+1})\) & polinomiale\\
		\addlinespace
			& & \(k = \Omicron(2^n)\) & \(\Omicron(n \cdot 2^n)\)  & superpolinomiale\\
		\lightrule
			Backtracking & \(\Omicron(2^n)\) & \multicolumn{1}{c}{-} & \(\Omicron(2^n)\) & esponenziale\\
		\lightrule
			Memoization & \(\Omicron(nk)\) & \(t = \ceil{\log k}\) & \(\Theta(n\dots 2^k)\) & esponenziale\\
		\bottomrule
	\end{tabular}
\end{table}

\section{Problemi fortemente, debolmente {\NP}-completi}

\begin{definition}[dimensioni del problema]
Dato un problema decisionale \(R\) e una sua istanza \(I\).
La \alert{dimensione \(d\)} di \(I\) è la lunghezza della stringa che codifica \(I\).
Il \alert{valore \texttt{\#}} è il più grande numero intero che appare in \(I\).
\end{definition}

\begin{table}[htbp]\centering
	\caption{Problemi decisionali e relative grandezze}
	\label{tab:problem-dimensions}
	\begin{tabular}{@{} l *{3}{l} @{}}
		\toprule
			\textbf{Nome} & \multicolumn{1}{c}{\textbf{Istanza} \(I\)} & \multicolumn{1}{c}{\textbf{No.\ più grande} \texttt{\#}} & \multicolumn{1}{c}{\textbf{Dimensione} \(d\)} \\
		\midrule
			{\subSetSumProblem} & \(\{n, k, A\}\)    & \(\max\{n, k, \max(A)\)\} & \(\Omicron(n \log\texttt{\#})\)\\
		\lightrule
			{\cliqueProblem}    & \(\{n, m, k, G\}\) & \(\max\{n,m,k\}\) & \(\Omicron(n + m + \log\texttt{\#})\)\\
		\lightrule
			{\tsp} & \(\{n,k,d\}\) & \(\max\{n, k, \max(d)\}\) & \(\Omicron(n^2 \log\texttt{\#})\)\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{definition}[problema fortemente \NP-completo]
Sia \(R_p\) il problema \(R\) ristretto a quei dati d'ingresso per i quali il più grande valore da rappresentare è limitato superiormente da \(p(d)\), con \(p\) funzione polinomiale in \(d\).
\(R\) è \alert{fortemente \NP-completo} se \(R_p\) è \NP-completo.
\end{definition}

\begin{observation}
Il problema cricca ({\cliqueProblem}), ad esempio, è fortemente \NP-completo, mentre quello della somma di sottoinsieme {\subSetSumProblem} non lo è.
\end{observation}

\begin{definition}[problema debolmente \NP-completo]
Se un problema \NP-completo non è fortemente \NP-completo, allora è \alert{debolmente \NP-completo}.
\end{definition}

\subsection{Esempio di problema debolmente \NP-completo}

\subsubsection{Somma di sottoinsiemi (\subSetSumProblem)}

\paragraph{Definizione del problema}
Dati un vettore \(A\) contenente \(n\) interi positivi ed un intero positivo \(k\), \alert{esiste} un sottoinsieme \(S \subseteq \{1 \dots n\}\) tale che \(\sum_{i \in S} a[i] = k\)?

\begin{proof}[Dimostrazione. Somma di sottoinsieme è debolmente \NP-completo]
\(\forall A[i] \leqslant k\) (valori più grandi di \(k\) vanno esclusi).
Se \(k = \Omicron(n^c)\), allora \(\texttt{\#} = \max\{n, k, a_1, \dots, a_n\} = \Omicron(n^c)\).
La soluzione basata su programmazione dinamica ha complessità \(\Omicron(nk) = \Omicron(n^{c+1})\), quindi in {\PTIME}.
Possiamo dedurne che {\subSetSumProblem} non è fortemente \NP-completo.
\end{proof}

\begin{note}
Il problema di {\subSetSumProblem} è debolmente \NP-completo, in quanto limitando le dimensioni dei dati in ingresso il problema diventa polinomiale.
\end{note}

\subsection{Algoritmi pseudo-polinomiali}

\begin{definition*}[Algoritmo pseudo-polinomiale]
Un algoritmo che risolve un certo problema \(R\), per qualsiasi dato \(I\) d'ingresso, in tempo \(p(\texttt{\#},d)\), con \(p\) funzione polinomiale in \texttt{\#} e \(d\), ha complessità \alert{pseudo-polinomiale}.
\end{definition*}

\begin{observation}
Gli algoritmi per {\subSetSumProblem} basati su programmazione dinamica e memoization sono pseudo-polinomiali.
\end{observation}

\begin{theorem*}
Nessun problema fortemente \NP-completo può essere risolto da un algoritmo pseudo-polinomiale, a meno che non sia \(\PTIME = \NP\).
\end{theorem*}

\subsection{Esempi di problemi fortemente \NP-completi}

\subsubsection{Cricca (\cliqueProblem)}

\paragraph{Definizione del problema}
Dati un grafo non orientato ed un intero \(k\), esiste un sottoinsieme di almeno \(k\) nodi tutti mutuamente adiacenti?

\begin{proof}[Dimostrazione. {\cliqueProblem} è fortemente \NP-completo]
\(k \leqslant n\) (altrimenti la risposta è \False), \(\texttt{\#} = \max\{n,m,k\}\), in quanto \(k \leqslant n\) possiamo semplificare \(\texttt{\#} = \max\{n,m\}\), \(d = \Omicron(n + m + \log\texttt{\#}) = \Omicron(n + m)\).
Quindi \(\texttt{\#} = \max\{n, m\}\) è limitato superiormente da \(\Omicron(n + m)\).
Possiamo dedurne che il problema ristretto è identico a {\cliqueProblem}, che è \NP-completo.
\end{proof}

\subsubsection{Commesso viaggiatore (\tsp)}

\paragraph{Definizione del problema}
Date \(n\) città e una matrice simmetrica d di distanze positive, dove \(d[i][j]\) è la distanza fra \(i\) e \(j\), trovare un percorso che, partendo da una qualsiasi città, attraversi ogni città esattamente una volta e ritorni alla città di partenza, in modo che la distanza totale percorsa sia minima.

\begin{proof}[Dimostrazione per assurdo che {\tsp} è fortemente \NP-completo]
Per assurdo, supponiamo che {\tsp} sia debolmente \NP-completo.
Allora esiste una soluzione pseudo-polinomiale.
Usiamo questa soluzione per risolvere un problema \NP-completo in tempo polinomiale, il che è assurdo a meno che \(\PTIME = \NP\).
\end{proof}

\subsubsection{Circuito hamiltoniano (\hamiltonianCircuit)}

\paragraph{Definizione del problema}
Dato un grafo non orientato \(G\), esiste un circuito che attraversi ogni nodo una e una sola volta?

\paragraph{Considerazioni}
Il problema {\hamiltonianCircuit} è \NP-completo.
\'{E} uno dei 21 problemi elencati nell'articolo di Karp.

\begin{proof}[Dimostriamo che {\tsp} è fortemente \NP-completo.]
Sia \(G = (V,E)\) un grafo non orientato.
Definiamo una matrice di distanze a partire da \(G\).
\[
	d[i][j] =
	\begin{dcases}
		1 & (i,j) \in E\\
		2 & (i,j) \not\in E\\
	\end{dcases}
\]

Il grafo \(G\) ha un circuito hamiltoniano se e solo se è possibile trovare un percorso da commesso viaggiatore di costo \(n\).

\paragraph{Simmetria fra {\tsp} e {\hamiltonianCircuit}}
Se esistesse un algoritmo pseudopolinomiale \(A\) per {\tsp}, {\hamiltonianCircuit} potrebbe essere risolto da \(A\) in tempo polinomiale.
\end{proof}

\subsubsection{Partizione (\partition)}

\paragraph{Definizione del problema}
Dato un vettore \(A\) contenente \(n\) interi positivi, esiste un sottoinsieme \(S \subseteq \{1 \dots n\}\) tale che \(\sum_{i \in S} A[i] = \sum_{i \in S} A[i]\)?

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\coordinate (t) at (.25,+.6);
		\foreach \num in {0,1,...,5}{
			\node[font=\tiny] at (t) {\num};
			\coordinate (t) at ($(t) + (0.6,0)$);
		}

		\coordinate (s) at (0,0);
		\foreach \num in {14,6,12,3,7,2}{
			% \node[draw, thick, rectangle, minimum size=0.5cm] at (s) {\num};
			\node[cell, minimum width = 0.6cm, minimum height = 0.6cm] at (s) {\num};
			\coordinate (s) at ($(s) + (0.6,0)$);
		}
	\end{tikzpicture}
\end{figure}

\paragraph{Conclusioni}
Il problema {\partition} è debolmente \NP-completo perché è possibile ridurlo a {\subSetSumProblem} scegliendo come valore \(k\) la metà di tutti i valori presenti:
\[
	k = \frac{\sum_{i=1}^{n} A[i]}{2} = \frac{44}{2} = 22
\]

\subsubsection{3-Partizione (\treePartition)}

Dati \(3n\) interi \(\{a_1, \dots, a_{3n}\}\) esiste una partizione in \(n\) triple \(T_1, \dots, T_n\), tale che la somma dei tre elementi di ogni \(T_j\) sia la stessa, per \(1 \leqslant j \leqslant n\)?

\paragraph{Conclusioni}
Il problema {\treePartition} è fortemente \NP-completo perché non esiste un algoritmo pseudopolinomiale per risolverlo.

\section{Algoritmi di approssimazione}

Facciamo una piccola premessa.
I problemi più interessanti sono in forma di ottimizzazione.
Se il problema di decisione è \NP-completo, non sono noti algoritmi polinomiali per il problema di ottimizzazione.
Esistono algoritmi polinomiali che trovano soluzioni ammissibili più o meno vicine a quella ottima.

\begin{definition*}[Algoritmi di approssimazione]
Se è possibile dimostrare un limite superiore/inferiore al rapporto fra la soluzione trovata e la soluzione ottima, allora tali algoritmi vengono detti \alert{algoritmi di approssimazione}.
\end{definition*}

\begin{definition*}[\(\alpha(n)-approssimazione\)]
Dato un problema di ottimizzazione con funzione costo non negativa \(c\), un algoritmo si dice di \alert{\(\alpha(n)\)-approssimazione} se fornisce una soluzione ammissibile \(x\) il cui costo \(c(x)\) non si discosta dal costo \(c(x^*)\) della soluzione ottima \(x^*\) per più di un fattore \(\alpha(n)\), per qualunque input di dimensione \(n\):
\begin{alignat}{4}
	c(x^*) \leqslant c(x)           &\leqslant \alpha(n) c(x^*) &\quad& \alpha(n) > 1 \quad (\text{Minimizzazione})\\
	\alpha(n) c(x^*) \leqslant c(x) &\leqslant c(x^*)           &\quad& \alpha(n) < 1 \quad (\text{Massimizzazione})
\end{alignat}
\end{definition*}

\begin{observation}
\(\alpha(n)\) può essere una costante, valida per tutti gli \(n\).
\end{observation}

\begin{note}
Identificare un valore \(\alpha(n)\) e dimostrare che l'algoritmo lo rispetta è ciò che rende un buon algoritmo un algoritmo di approssimazione.
\end{note}

\subsection{Bin packing}

\paragraph{Definizione del problema}
Dati un vettore \(A\) contenente \(n\) interi positivi (i \alert{volumi} degli \alert{oggetti}) e un intero positivo \(k\) (la \alert{capacità} di una \alert{scatola}, tale che \(\forall i: A[i] \leqslant k\)), si vuole trovare una partizione di \(\{1, \dots, n\}\) nel minimo numero di sottoinsiemi disgiunti (\enquote{scatole}) tali che \(\sum_{i \in S} A[i] \leqslant k\) per ogni insieme \(S\) della partizione.

Dato il seguente vettore e un intero \(k = 8\) come risolvereste il problema?
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\coordinate (t) at (.25,+.5);
		\foreach \num in {0,1,...,6}{
			\node[font=\tiny] at (t) {\num};
			\coordinate (t) at ($(t) + (0.5,0)$);
		}

		\coordinate (s) at (0,0);
		\foreach \num in {3,7,2,5,4,3,5}{
			\node[cell] at (s) {\num};
			\coordinate (s) at ($(s) + (0.5,0)$);
		}
	\end{tikzpicture}
\end{figure}

\subsubsection{Appoccio ingordo per bin packing}

\paragraph{Algoritmo first-fit}
Gli oggetti sono considerati in un ordine qualsiasi e ciascun oggetto è assegnato alla prima scatola che lo può contenere, tenuto conto di quanto spazio è stato occupato della stessa.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}

		\coordinate (s) at (0,0);
		\foreach \num in {{3,2,3},7,5,4,5}{
			\node[cell, minimum width = 1.3cm] at (s) {\num};
			\coordinate (s) at ($(s) + (1.3,0)$);
		}
	\end{tikzpicture}
\end{figure}

Sia \(N > 1\) il numero di scatole usare da {\firstFit} (se \(N=1\), l'algoritmo è ottimale).
Il numero minimo di scatole \(N^*\) è limitato da:
\[
	N^{*} \geqslant \frac{\sum_{i=1}^n A[i]}{k} = \frac{29}{8} = 3.625
\]
non possono esserci due scatole riempite meno della metà:
\[
	N < \frac{\sum_{i=1}^n A[i]}{k/2} = \frac{29}{8/2} = 7.250
\]
abbiamo quindi:
\[
	N < \frac{\sum_{i=1}^n A[i]}{k/2} = 2 \frac{\sum_{i=1}^n A[i]}{k} \leqslant 2N^* = \alpha(n) N^{*}
\]
che implica \(\alpha(n) = 2\).

\paragraph{Algoritmo first-fit decreasing}
Se consideriamo gli oggetti in ordine non descrescente è possibile dimostrare un risultato migliore per questo algoritmo.
\[
	N < \frac{17}{10} N^{*} + 2
\]

Nella variante \textsc{ffd} (First-fit decreasing): gli oggetti sono considerati in ordine non decrescente
\[
	N < \frac{11}{9} N^{*} + 4
\]
Queste sono dimostrazioni di limiti superiori per il fattore \(\alpha(n)\), per casi particolari l'approssimazione può essere migliore.

\subsection{Commesso viaggiatore con disuguaglianze triangolari ({\deltaTsp})}

\paragraph{Definizione del problema}
Siano date \(n\) città e le distanze (positive) \(d[i][j]\) tra esse, \alert{tale per cui vale la regola delle diseguaglianze triangolari}:
\[
	d[i][j] \leqslant d[i][k] + d[k][j] \quad \forall i,j,k: \quad 1 \leqslant i,j,k \leqslant n
\]
Trovare un percorso che, partendo da una qualsiasi città, attraversi ogni città esattamente una volte e ritorni alla città di partenza, in modo che la distanza complessiva percorsa sia minima.

\begin{figure}[H]
	\begin{subfigure}[t]{.5\linewidth}\centering
		\includegraphics[page=1]{triangle-inequality}
		\caption{con diseguaglianza triangolare}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\linewidth}\centering
		\includegraphics[page=2]{triangle-inequality}
		\caption{senza diseguaglianza triangolare}
	\end{subfigure}
\end{figure}

\begin{proof}[Dimostriamo che {\hamiltonianCircuit} \(\leqslant_p\) {\deltaTsp}]
Sia \(G = (V, E)\) un grafo non orientato. Definiamo una matrice delle distanze a partire da \(G\)
\[
	d[i][j] =
	\begin{dcases}
		1 & (i,j) \in E\\
		2 & (i,j) \not\in E\\
	\end{dcases}
\]
Il grafo \(G\) ha un circuito hamiltoniano se e solo se è possibile trovare un percorso da commesso viaggiatore lungo \(n\).
Valgono le diseguaglianze triangolari:
\[
	d[i][j] \leqslant 2 \leqslant d[i][k] + d[k][j]
\]
\end{proof}

\subsubsection{Commesso viaggiatore come circuito hamiltoniano pesato}

Interpretiamo {\deltaTsp} come il problema di trovare un circuito hamiltoniano di peso minimo su un grafo completo.

\begin{figure}[H]\centering
	\hfill
	\begin{subfigure}[c]{.22\linewidth}\centering
		\includegraphics{tsp-greedy}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[c]{.22\linewidth}\centering
		\includegraphics[page=9, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[c]{.22\linewidth}\centering
		\includegraphics[page=5, width=\linewidth]{tsp-local}
	\end{subfigure}%
	\hfill\null
	\caption{Il costo totale risulta 21. Il costo totale risulta 19.}
\end{figure}

\subsubsection{Algoritmo di approssimazione per {\deltaTsp}}

Se si considera un circuito hamiltoniano e si cancella un suo arco, si ottiene un albero di copertura.

\begin{figure}[H]\centering
	\includegraphics[page=1, width=0.5\linewidth]{toscana}
\end{figure}

\begin{theorem*}
Qualunque circuito hamiltoniano \(\pi\) ha costo \(c(\pi)\) superiore al costo \(mst\) di albero di copertura di peso minimo, ovvero \(mst < c(\pi)\).
\end{theorem*}

\begin{proof}[Dimostrazione per assurdo.]
Supponiamo che esista un circuito hamiltoniano \(\pi\) di costo \(c(\pi) \leqslant mst\).
Togliamo un arco, otteniamo un albero di copertura con peso inferiore \(mst' < c(\pi) \leqslant mst\).
Contraddizione, visto che \(mst\) è il costo minimo fra tutti gli alberi di copertura.
\end{proof}

\subsubsection{Algoritmo per {\deltaTsp}}

Si individua un minimo albero di copertura di peso \(mst\) e se ne percorrono gli archi due volte, prima in un senso e poi nell'altro.
In questo modo, si visita ogni città almeno una volta.
La distanza complessiva di tale circuito è uguale a \(2 \cdot mst\).
Ma non è un circuito hamiltoniano!

\begin{figure}[H]\centering
	\begin{subfigure}[t]{.5\linewidth}\centering
		\includegraphics[page=2, width=.8\linewidth]{toscana}
		\caption{con diseguaglianza triangolare}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\linewidth}\centering
		\includegraphics[page=3, width=.8\linewidth]{toscana}
		\caption{senza diseguaglianza triangolare}
	\end{subfigure}
\end{figure}

Si evita di passare per città già visitate, saltandole.
Per la diseguaglianza triangolare, il costo \(c(\pi)\) del circuito così ottenuto è inferiore o uguale a \(2 \cdot mst\).
Concludendo, \(c(\pi) \leqslant 2 \cdot mst < 2 \cdot 2 \cdot c(\pi^*)\) implica che \(\alpha(n) = 2\), dove \(c(\pi^*)\) è il costo del circuito hamiltoniano ottimo.

\begin{figure}[H]\centering
	\begin{subfigure}[t]{.5\linewidth}\centering
		\includegraphics[page=4, width=.8\linewidth]{toscana}
		\caption{con diseguaglianza triangolare}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\linewidth}\centering
		\includegraphics[page=5, width=.8\linewidth]{toscana}
		\caption{senza diseguaglianza triangolare}
	\end{subfigure}
\end{figure}

\subsubsection{Discussione sulla complessità dell'algoritmo per {\deltaTsp}}

La complessità dell'algoritmo è pari a \(\Omicron(n^2 \log n)\).
I fattori che contribuiscono sono:
\begin{itemize}
	\item \(\Omicron(n^2 \log n)\) per l'algoritmo di Kruskal;
	\item \(\Omicron(n)\) per la visita in profondità del MST con \(2n\) archi.
\end{itemize}
Tuttavia esistono grafi \enquote{perversi} per cui il fattore di approssimazione tende al valore 2.
L'algoritmo di Christofides (1976) ha un fattore di approssimazione di 3/2, il migliore risultato al momento.

\subsubsection{Non approssimabilità di {\tsp}}

Non esiste alcun algoritmo di \(\alpha(n)\)-approssimazione per {\tsp} tale che \(c(x') \leqslant sc(x^*)\), con \(s \geqslant 2\) intero positivo, a meno che non sia \(\PTIME = \NP\).

\begin{note}
\(\Delta-tsp\) è un problema approssimabile, ma il problema generale non lo è.
\end{note}

\section{Algoritmi euristici}

Quando si è presi dalla disperazione a causa della enorme difficoltà di un problema di ottimizzazione {\NP}-hard, si può ricorrere ad algoritmi \enquote{euristici} che forniscono una soluzione ammissibile, che non è necessariamente ottima, né non necessariamente approssimata.

Le tecniche che è possibile utilizzare in questi casi sono l'approccio ingordo e la ricerca locale.

\newpage
\subsection{Approccio ingordo per {\tsp}}

\subsubsection{Algoritmo \enquote{Shortest edges first}}

Ordiniamo gli archi per pesi non decrescenti e aggiungiamo archi alle soluzioni seguendo questo ordine finché non sono stati aggiunti \(n-1\) archi, dove \(n\) è il numero di nodi.
Per poter aggiungere un arco, occorre verificare che per ciascuno dei suoi nodi non siano stati già scelti due archi e che non si formino circuiti (questo lo si fa tramite l'utilizzo della struttura dati {\mfSet}).
A questo punto, si è trovata una catena Hamiltoniana.
Si chiude il circuito aggiungendo l'arco tra i due nodi estremi della catena.

\begin{figure}[H]\centering
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=1, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=2, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=3, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=4, width=\linewidth]{tsp-pentagon}
	\end{subfigure}

	\vspace{10pt}

	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=5, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=6, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=7, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=8, width=\linewidth]{tsp-pentagon}
	\end{subfigure}%

	\caption{"pentagon"}
\end{figure}

\begin{algorithm}[H]
	\caption{Risoluzione del commesso viaggiatore con algoritmo ingordo}
	\input{greedyTsp}
\end{algorithm}

\newpage
\subsubsection{Algoritmo \enquote{Nearest Neighbor}}

Possiamo usare un approccio diverso.
Si parte da una città, si seleziona come prossima città quella più vicina e si va avanti così, evitando città già visitate.
Quando si sono visitate tutte le città, si torna alla città di partenza.

Così facendo possiamo lavorare direttamente sulla matrice senza usare strutture di appoggio.

\begin{figure}[H]\centering
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=1, width=\linewidth]{tsp-nn}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=2, width=\linewidth]{tsp-nn}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=3, width=\linewidth]{tsp-nn}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=4, width=\linewidth]{tsp-nn}
	\end{subfigure}%

	\vspace{10pt}

	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=5, width=\linewidth]{tsp-nn}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=6, width=\linewidth]{tsp-nn}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=7, width=\linewidth]{tsp-nn}
	\end{subfigure}%
	\begin{subfigure}{.25\linewidth}\centering
		\includegraphics[page=8, width=\linewidth]{tsp-nn}
	\end{subfigure}%

	\caption{"nn"}
\end{figure}

\subsubsection{Discussione sulla complessità dell'approccio ingordo per {\tsp}}

Il costo computazionale dell'algoritmo \enquote{Shortest edges first} ha un costo computazionale di \(\Omicron(n^2 \log n)\), dove \(\log n\) è dovuto all'ordinamento degli archi; mentre l'algoritmo \enquote{Nearest Neighbor} ha un costo di \(\Omicron(n^2)\).

La soluzione così ottenuta si può utilizzare come base di partenza per un algoritmo branch-\&-bound, può essere migliorata ancora tramite ricerca locale.

\subsection{Approccio ricerca locale per {\tsp}}

\subsubsection{Algoritmo di ricerca locale}

Sia \(\pi\) un circuito Hamiltoniano del grafo completo derivante dal problema {\tsp}.
Si consideri il seguente intorno:
\(I_2(\pi) =\) \{\(\pi'\): \(\pi'\) è ottenuto da \(\pi\) cancellando due archi non consecutivi del circuito e sostituendoli con due archi esterni al circuito\}

\begin{note}
La cardinalità di \(I_(\pi)\) è \(\abs{I_(\pi)} = \frac{n(n-1)}{2}-n\), in quanto ci sono \(\frac{n(n-1)}{2}\) coppie di archi del circuito e \(n\) di esse sono consecutive.

Una volta spezzato il circuito, esiste un solo modo per riconnetterlo.
\end{note}

\begin{figure}[H]\centering
	\begin{subfigure}[c]{.4\linewidth}\centering
		\includegraphics[page=1, width=.5\linewidth]{tsp-local}\hfill
		\includegraphics[page=2, width=.5\linewidth]{tsp-local}
		\caption{Costo: 25.}
	\end{subfigure}%
	\begin{subfigure}[c]{.4\linewidth}\centering
		\includegraphics[page=3, width=.5\linewidth]{tsp-local}\hfill
		\includegraphics[page=4, width=.5\linewidth]{tsp-local}
		\caption{Costo: 23.}
	\end{subfigure}%
	\begin{subfigure}[c]{.20\linewidth}\centering
		\includegraphics[page=5, width=\linewidth]{tsp-local}
		\caption{Costo: 19.}
	\end{subfigure}%
	\caption{Applicazione dell'algoritmo di ricerca locale.}
\end{figure}

\subsubsection{Discussione sulla complessità dell'approccio di ricerca locale per {\tsp}}

Il costo per esaminare \(I_2(\pi)\) è \(\Omicron(n^2)\).

\section{Algoritmi branch\&bound}

Per risolvere un problema di ottimizzazione \NP-arduo, si può modificare la procedura \enumeration, vista nel capitolo sul backtracking, in modo da \enquote{potare} certe sequenze di scelte che di rivelino incapaci di generare la soluzione ottima.
Cerchiamo quindi di analizzare tutto lo spazio delle soluzioni, evitando certe sequence di scelte che facciano diminuire il costo della soluzione parziale costruita.

Facciamo una serie di assunzioni senza perdere troppa generalità:
\begin{itemize}
	\item problema di ottimizzazione;
	\item ogni sequenza di scelte abbia costo non negativo;
	\item ogni scelta, aggiunta alle scelte già effettuate, non faccia diminuire il costo della soluzione parziale così ottenuta.
\end{itemize}

\begin{algorithm}[H]
	\caption{Ripasso della procedura di enumerazione delle soluzioni}
	\input{enumeration}
\end{algorithm}

Cerchiamo i limiti (superiore ed inferiore) della soluzione minima.

\subsubsection{Limite superiore}

Durante l'enumerazione, si mantengono informazioni sulla miglior soluzione ammissibile (\(minSol\)) ed il suo costo (\(minCost\)).
\(minCost\) costituisce un limite superiore (\foreign{upper bound}) per il costo della soluzione minima.

\subsubsection{Limite inferiore}

Si supponga di avere a disposizione una opportuna funziona \enquote{lower bound} {\lowerBound{\(\angleled{dati\ problema}\), S, i, \(\angleled{dati\ parziali}\)}}, che dipenda dalla sequenza di scelte fatte in precedenza \(S[1 \dots i]\), e che garantisca che tutte le soluzioni ammissibili generabili facendo nuove scelte abbiano costo \(\geqslant \lowerBound\).

\begin{figure}[H]
	\centering
	\includegraphics{branchAndBound-1}
	\caption{Se \textsf{lb}(\(S\),\(i\)) è maggiore o uguale a \(minCost\), allora si può evitare di generare ed esplorare il sottoalbero delle scelte radicato in tal nodo, effettuando così una \textbf{potatura}.}
\end{figure}

Questo metodo non migliora la complessità (superpolinomiale) della procedura \enumeration, ma nella pratica ne abbassa di molto il tempo di esecuzione.
Tutto dipende dalla funzione \lowerBound, che deve approssimare il più possibile il costo della soluzione ottima.
Il limite superiore è dato dal \(minCost\).

\begin{algorithm}[H]
	\caption{Somma di sottoinsiemi}
	\input{branchAndBound}
\end{algorithm}

\subsection{Approccio branch-\&-Bound per {\tsp}}

Applichiamo questo ragionamento al problema del commesso viaggiatore.

Sia \(n\) il numero delle città, e \(d[h][k]\) la distanza, intera e non negativa, fra le città \(h\) e \(k\).
Al passo \(i\)-esimo sono state fatte le scelte \(S[1 \dots i]\) prese dall'insieme \(\{1, \dots, n\}\).
Un percorso ammissible che \enquote{espande} \(S[1 \dots i]\) deve
\begin{enumerate}[label=\arabic*)]
	\item attraversare le città \(S[1 \dots i]\);
	\item passare da \(S[i]\) ad una qualsiasi delle rimanenti \(n-1\) città;
	\item attraversare queste ultime città in un ordine qualsiasi;
	\item da una di queste ritornare a \(S[1]\).
\end{enumerate}

Facciamo un po' di calcoli.

La distanza percorsa finora è \(cost[i]\), ossia il costo che ho sostenuto per fare i primi \(i\) passi.
\[
	cost[i] =
	\begin{dcases}
		0                               & i = 1\\
		cost[i-1] + d[ S[i-1] ][ S[i] ] & i > 1\\
	\end{dcases}
\]


Il limite inferiore (\foreign{lower bound}) della distanza per "uscire" da \(S[i]\) (ha un costo di \(O(n)\)):
\[
	\out  = \min_{h \notin S} \{ d[S[i], h] \}
\]

Il limite inferiore (\foreign{lower bound}) della distanza per tornare a \(S[1]\) (ha un costo di \(O(n)\)):
\[
	\back = \min_{h \notin S} \{ d[h, S[1]] \}
\]

Il limite inferiore della distanza percorsa per attraversare una qualsiasi di queste ultime città \(h\) delle \(n - i\) città, provenendo da (e dirigendosi verso) una città non compresa in \(S[2\dots n]\) (ha un costo di \(O(n^3)\)).
\[
	\forall h \not\in S: \transfer = \min_{p,q \not\in S[2\dots i-1]}\{d[p, h] + d[h, q] \colon h \Neq p \Neq q \}
\]

Se posso effettuare ancora delle scelte (\(i < n\)), è possibile calcolare un possibile limite inferiore \(\lowerBound{d, S, i}\) calcolando la somma:
\begin{itemize}
	\item del costo \(cost[i]\) per arrivare al nodo \(S[i]\), già speso;
	\item metà del costo ottenuto sommando:
	\begin{itemize}
		\item il limite inferiore \(out\) del costo per andare dal nodo \(S[i]\) ad un qualunque altro nodo;
		\item il limite inferiore per attraversare i nodi non contenuti in \(S\);
		\item il limite inferiore \(\back\) del costo per tornare al nodo \(S[i]\) da un qualunque altro nodo.
	\end{itemize}
\end{itemize}
\[
	\lowerBound(d,S,i) = cost[i] + \ceil*{\frac{\out + \sum_{h \not\in S} \transfer[h] + \back}{2}}
\]

\begin{algorithm}[H]
	\caption{Appoccio branch\&bound al problema del commesso viaggiatore}
	\input{bbTsp}
\end{algorithm}

\subsubsection{Precisazione sull'algoritmo}

\(\minCost\) è una variabile globale.
Invece di inizializzarla a \(+\infty\) possiamo sceglie una permutazione a caso.
Ad esempio la permutazione 1-2-3-4-5 ha un costo pari a \(21\).
Per evitare che lo stesso circuito sia generato più volte, si parte da un nodo fissato (ad esempio 1).

\begin{figure}[H]\centering
	\includegraphics[width=\textwidth]{branchAndBound-2}
\end{figure}
In questo semplice esempio è stato possibile \enquote{potare} 42 nodi su 65.

\subsubsection{Possibili miglioramenti}

Sono possibili diversi miglioramenti:
\begin{itemize}
	\item è possibile variare l'ordine di visita dell'albero delle scelte;
	\item è possibile variare il meccaniscmo di ramificazione (\foreign{brancing});
	\item è possibile cercare dei limiti inferiori più stretti.
\end{itemize}

\section{Riassumendo}

Abbiamo prima trattato di algoritmi pseudopolinomiali vedendo più soluzioni per il problema di somma di sottoinsiemi ({\subSetSumProblem}).
Abbiamo visto che è possibile risolverlo utilizzando più tecniche.
Tramite la programmazione dinamica abbiamo ottenuto una complessità di \(\Theta(nk)\), tramite backtracking una complessità di \(\Omicron(2^n)\) ed infine tramite memoization una complessità di \(\Omicron(nk)\).
Sia tramite la programmazione dinamica che la memoization, che abbiamo visto essere un miglioramento della stessa, la complessità dell'algoritmo dipende dalla dimensione dell'input.

Abbiamo poi definito i problemi fortemente e debolmente {\NP}-completi.
Abbiamo illustrato vari problemi e abbiamo dimostrato le varie equivalenze fra di essi.
Per illustrare i capitoli successivi abbiamo preso come modello il problema del commesso viaggiatore {\tsp}.

Nell'affrontare l'argomento degli algoritmi di approssimazione abbiamo dovuto definire un problema ristretto rispetto a {\tsp}, {\deltaTsp}, in quanto esso risulta non approssimabile.

Abbiamo poi affrontato il capitolo degli algoritmi euristici vedendo due approcci ingordi per {\tsp}.
L'algoritmo \enquote{Shortest edges first} ha una complessità di \(\Omicron(n^2 \log n)\), dove \(\log n\) è dovuto all'ordinamento degli archi; mentre \enquote{Nearest Neighbor} ha un costo di \(\Omicron(n^2)\).
Abbiamo poi migliorato ulteriolmente l'algoritmo utilizzando l'approccio di ricerca locale.
Tuttavia non siamo riusciti ad ottenere un miglioramento nella complessità che è risultata comunque di \(\Omicron(n^2)\).

Infine abbiamo visto un'ultima tipologia di algoritmi, quelli del tipo \enquote{branch-\&-bound}.

\end{document}
